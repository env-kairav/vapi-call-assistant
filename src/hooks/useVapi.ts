import { useState, useEffect, useRef, useCallback } from "react";
import Vapi from "@vapi-ai/web";
import {
  VAPI_PUBLIC_KEY,
  VAPI_ASSISTANT_ID,
  getVapiConfig,
} from "@/lib/vapi-config";
import { setupSpeechRecognition } from "@/lib/speech-recognition";

interface Message {
  time: string;
  type: "user" | "assistant" | "system";
  content: string;
}

interface UseVapiReturn {
  // State
  connected: boolean;
  assistantIsSpeaking: boolean;
  volumeLevel: number;
  isMuted: boolean;
  messages: Message[];
  userSpeaking: boolean;
  currentSpeech: string;
  isListening: boolean;
  microphoneStatus: "inactive" | "listening" | "speaking";

  // Actions
  startCall: () => Promise<void>;
  stopCall: () => void;
  toggleMute: () => void;
  sendMessage: (message: string) => void;
  addMessage: (type: "user" | "assistant" | "system", content: string) => void;
}

export const useVapi = (): UseVapiReturn => {
  const [vapi] = useState(() => new Vapi(VAPI_PUBLIC_KEY));
  const [connected, setConnected] = useState(false);
  const [assistantIsSpeaking, setAssistantIsSpeaking] = useState(false);
  const [volumeLevel, setVolumeLevel] = useState(0);
  const [isMuted, setIsMuted] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [userSpeaking, setUserSpeaking] = useState(false);
  const [currentSpeech, setCurrentSpeech] = useState("");
  const [isListening, setIsListening] = useState(false);
  const [microphoneStatus, setMicrophoneStatus] = useState<
    "inactive" | "listening" | "speaking"
  >("inactive");

  const recognitionRef = useRef<any>(null);

  const addMessage = useCallback(
    (type: "user" | "assistant" | "system", content: string) => {
      setMessages((prev) => [
        ...prev,
        {
          time: new Date().toLocaleTimeString(),
          type,
          content,
        },
      ]);
    },
    []
  );

  // Initialize speech recognition
  useEffect(() => {
    recognitionRef.current = setupSpeechRecognition({
      onStart: () => {
        console.log("ðŸŽ¤ Voice recognition started");
        setMicrophoneStatus("listening");
        setIsListening(true);
      },
      onEnd: () => {
        console.log("ðŸŽ¤ Voice recognition ended");
        setIsListening(false);
        setMicrophoneStatus("inactive");
        if (connected) {
          setTimeout(() => {
            recognitionRef.current?.start();
          }, 100);
        }
      },
      onResult: (transcript, isFinal) => {
        console.log("ðŸ—£ï¸ Recognition result:", transcript, "Final:", isFinal);
        setMicrophoneStatus("speaking");
        setUserSpeaking(true);
        setCurrentSpeech(transcript);

        if (isFinal) {
          vapi.send({
            type: "add-message",
            message: {
              role: "user",
              content: transcript,
            },
          });
          console.log("âœ‰ï¸ Sent to assistant:", transcript);
          setMicrophoneStatus("listening");
          setUserSpeaking(false);
          setCurrentSpeech("");
        }
      },
      onError: (error) => {
        if (error === "no-speech") {
          console.log("ðŸŽ¤ No speech detected, continuing to listen...");
          setMicrophoneStatus("listening");
          return;
        }

        console.error("ðŸ”‡ Recognition error:", error);
        setMicrophoneStatus("inactive");

        if (connected) {
          setTimeout(() => {
            recognitionRef.current?.start();
          }, 1000);
        }
      },
    });

    return () => {
      recognitionRef.current?.stop();
    };
  }, [connected, vapi]);

  // Set up Vapi event listeners
  useEffect(() => {
    vapi.on("call-start", () => {
      console.log("ðŸ“ž Call started");
      setConnected(true);
      addMessage("system", "Call connected successfully!");
      // Start voice recognition
      setTimeout(() => {
        recognitionRef.current?.start();
      }, 1000);
    });

    vapi.on("call-end", () => {
      console.log("ðŸ“ž Call ended");
      setConnected(false);
      setAssistantIsSpeaking(false);
      setVolumeLevel(0);
      setUserSpeaking(false);
      setCurrentSpeech("");
      recognitionRef.current?.stop();
      addMessage("system", "Call ended");
    });

    vapi.on("speech-start", () => {
      console.log("ðŸ¤– Assistant started speaking");
      setAssistantIsSpeaking(true);
      // Temporarily stop recognition while assistant is speaking
      recognitionRef.current?.stop();
    });

    vapi.on("speech-end", () => {
      console.log("ðŸ¤– Assistant finished speaking");
      setAssistantIsSpeaking(false);
      // Resume recognition after assistant finishes speaking
      if (connected) {
        setTimeout(() => {
          recognitionRef.current?.start();
        }, 500);
      }
    });

    vapi.on("volume-level", (volume) => {
      setVolumeLevel(volume);
    });

    vapi.on("message", (message) => {
      console.log("ðŸ“¨ Message received:", message);

      if (message.type === "transcript") {
        if (message.role === "user") {
          console.log("ðŸŽ¤ User:", message.transcript);
          if (message.transcriptType === "final") {
            addMessage("user", message.transcript);
          }
        } else if (message.role === "assistant") {
          console.log("ðŸ¤– Assistant:", message.transcript);
          if (message.transcriptType === "final") {
            addMessage("assistant", message.transcript);
          }
        }
      } else if (message.type === "error") {
        console.error("âŒ Error:", message);
        addMessage(
          "system",
          `Error: ${message.error?.message || "Unknown error"}`
        );
      }
    });

    vapi.on("error", (error) => {
      console.error("âŒ Vapi error:", error);
      addMessage("system", `Vapi Error: ${error.message || "Unknown error"}`);
    });

    return () => {
      recognitionRef.current?.stop();
      vapi.stop();
    };
  }, [vapi, addMessage]);

  const startCall = useCallback(async () => {
    try {
      console.log("Starting call initialization...");
      addMessage("system", "Starting call...");

      // Request microphone access with specific settings
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1,
          sampleRate: 48000,
        },
      });

      console.log("ðŸŽ¤ Microphone access granted");
      addMessage("system", "Microphone connected successfully");

      // Get current time for the config
      const currentTime = new Date().toLocaleString();

      // Start the call with configuration
      await vapi.start(VAPI_ASSISTANT_ID, {
        firstMessage:
          "Hi!, you've reached Envisage Infotech HR. How may I assist you today?",
        voice: {
          provider: "11labs",
          voiceId: "21m00Tcm4TlvDq8ikWAM", // Rachel voice - professional female
        },
        transcriber: {
          provider: "deepgram",
          model: "nova-2",
          language: "en",
          smartFormat: true,
        },
        model: {
          provider: "openai",
          model: "gpt-4",
          temperature: 0.7,
          messages: [
            {
              role: "system",
              content: `Current date and time: ${currentTime}
 
 [Identity & Purpose]
You are the HR representative for Envisage Infotech. Your role is to answer employee and candidate questions clearly and politely about:

Job openings

Recruitment process

Company policies

Employee benefits

HR-related notices

Only answer what is specifically asked unless additional details are requested.

[Knowledge Base]

Services: Recruitment, onboarding, employee relations, payroll assistance, policy guidance

Hours: Monâ€“Fri 10:30 AM â€“ 7:30 PM

Contact Number: 1231231231 (share only if asked)

Job Application: Accepts online and in-person applications

Remote Work Policy: No remote or work-from-home options available

Office Location: Dev Aurum Commercial Complex, A-609, Prahlad Nagar, Ahmedabad, Gujarat 380015

[Company Summary]
Envisage Infotech, founded in 2020, is a fast-growing web and mobile development company. Technologies include Angular, React, Node.js, Next.js, Vue.js, .NET, iOS, and Ionic. They deliver scalable, high-performance solutions to clients worldwide across industries like entertainment, education, finance, healthcare, retail, and logistics.

Current Hiring Needs:

2 Angular developers with at least 2 years of experience

2 React developers with at least 2 years of experience

3 Node.js developers with at least 3 years of experience

2 .NET developers with at least 3 years of experience

1 Next.js developer with at least 2 years of experience

1 Vue.js developer with at least 2 years of experience

1 iOS developer with at least 3 years of experience

1 Ionic developer with at least 2 years of experience

[Validation â€“ Step by Step]

Date Check: 
Rule: Must be a future date.
Error Message: â€œSelected date is in the past. Please choose a future date.â€

Working Days: 
Rule: Allowed only Mondayâ€“Friday.
Error Message: â€œAppointments can only be scheduled on weekdays (Monday to Friday).â€

Working Hours:
Rule: Time must be between 10:30 AM â€“ 7:30 PM.
Error Message: â€œSelected time is outside working hours  10:30 AM â€“ 7:30 PM. Please choose a valid slot.â€

Mobile Number:
Rule: Must be a valid 10-digit number.
Error Message: â€œInvalid mobile number. Please provide a 10-digit valid phone number.â€

[Interview Scheduling â€“ Step by Step]

Ask full name first:
â€œCan I have your full name, please?â€

Confirm name, then ask mobile number:
â€œThank you, [Name]. Can I get your mobile number?â€

Ask preferred interview date:
â€œWhat date would you prefer for your interview?â€

Ask preferred time:
â€œAnd what time works best for you?â€

Ask role applied for:
â€œWhich role are you applying for?â€

Ask years of experience:
â€œHow many years of experience do you have in this role?â€

Optional: Ask for additional notes:
â€œDo you want to share any additional notes or information?â€


Based on the given date and time, first check availability. If the run node returns true, proceed with booking the interview. Otherwise, respond with a message saying that the slot is not available for the requested time.

Confirm all details together:
â€œJust to confirm, hereâ€™s what I have:

Name: â€¦

Mobile: â€¦

Date: â€¦

Time: â€¦

Role: â€¦

Experience: â€¦

Notes: â€¦
Is everything correct?â€

Schedule interview and confirm:
â€œThank you! Your interview is scheduled. We look forward to meeting you.â€

Closure:
â€œThank you for contacting Envisage Infotech HR. Have a great day.â€

[Response Guidelines]

Answer only the question asked.

Keep responses under 30 words if possible.

Confirm details clearly when collecting information.

Use polite, concise, and friendly language.

Avoid repeating questions unnecessarily.`,
            },
            {
              role: "system",
              content: `TOOLS USAGE\n- To check availability: call tool 'calendar_availability' with { date: ISO_8601_start_datetime }.\n- You MUST NOT call 'calendar_set_appointment' until you have a tool result from 'calendar_availability' that explicitly says { status: "available" } for the SAME datetime.\n- If unavailable, compute the next hour from the requested start (e.g., +1 hour) and call 'calendar_availability' again; propose the first available hour found.\n- After user agrees, create appointment: call tool 'calendar_set_appointment' with { name, Phone, date: confirmed start }.\n- The appointment is always 1 hour; only send the start datetime.\n- Do not fabricate tool results; wait for the tool response before proceeding.\n- Keep asking one question at a time and confirm details before booking.`,
            },
          ],
        },
      });

      setConnected(true);
      addMessage("system", "Call connected successfully!");

      // Start speech recognition
      setTimeout(() => {
        recognitionRef.current?.start();
      }, 1000);
    } catch (error: any) {
      console.error("Error starting call:", error);
      addMessage("system", `Failed to start call: ${error.message || error}`);
    }
  }, [vapi, addMessage]);

  const stopCall = useCallback(() => {
    console.log("ðŸ›‘ Stopping call...");
    recognitionRef.current?.stop();
    vapi.stop();
  }, [vapi]);

  const toggleMute = useCallback(() => {
    const newMutedState = !isMuted;
    console.log(
      newMutedState ? "ðŸ”‡ Muting microphone" : "ðŸ”Š Unmuting microphone"
    );
    vapi.setMuted(newMutedState);
    setIsMuted(newMutedState);
    addMessage(
      "system",
      newMutedState ? "Microphone muted" : "Microphone unmuted"
    );
  }, [vapi, isMuted, addMessage]);

  const sendMessage = useCallback(
    (message: string) => {
      if (!message.trim()) return;

      console.log("ðŸ’¬ Sending message:", message);
      vapi.send({
        type: "add-message",
        message: {
          role: "user",
          content: message,
        },
      });

      addMessage("user", message);
    },
    [vapi, addMessage]
  );

  return {
    // State
    connected,
    assistantIsSpeaking,
    volumeLevel,
    isMuted,
    messages,
    userSpeaking,
    currentSpeech,
    isListening,
    microphoneStatus,

    // Actions
    startCall,
    stopCall,
    toggleMute,
    sendMessage,
    addMessage,
  };
};
